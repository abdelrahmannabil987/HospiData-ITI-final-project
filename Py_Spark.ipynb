{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17c73a5-5436-49d1-945e-73b41382c3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://efbaeac86e87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hospital-batch</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74b344a44c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HospitalBatchModel\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2c883e-e767-46c5-94eb-3fbe4a5c0092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/base\n",
      "/home/jovyan/hospital_1\n",
      "/home/jovyan/hospital_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir   = os.getcwd()         ,
    "dim_path   = os.path.join(base_dir, \"base\")\n",
    "h1_path    = os.path.join(base_dir, \"hospital_1\")\n",
    "h2_path    = os.path.join(base_dir, \"hospital_2\")\n",
    "\n",
    "print(dim_path)\n",
    "print(h1_path)\n",
    "print(h2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97cb16e2-5be8-429e-8c82-9e53c20d0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ServiceCatalog.csv', 'TriageLevels.csv', 'VisitType.csv', 'Wards.csv', 'Beds.csv', 'PayerTypes.csv', 'VisitClassification.csv', 'Doctor_Speciality.csv', 'OperationType.csv', 'Speciality.csv', 'VisitStatus.csv', 'Doctors.csv', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(dim_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb5da37-ff2e-411b-b7c2-1f51e5c355a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "speciality_df      = spark.read.csv(\n",
    "    os.path.join(dim_path, \"Speciality.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "visit_class_df     = spark.read.csv(\n",
    "    os.path.join(dim_path, \"VisitClassification.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "visit_type_df      = spark.read.csv(\n",
    "    os.path.join(dim_path, \"VisitType.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "visit_status_df    = spark.read.csv(\n",
    "    os.path.join(dim_path, \"VisitStatus.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "wards_df           = spark.read.csv(\n",
    "    os.path.join(dim_path, \"Wards.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "beds_df            = spark.read.csv(\n",
    "    os.path.join(dim_path, \"Beds.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "doctors_df         = spark.read.csv(\n",
    "    os.path.join(dim_path, \"Doctors.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "doctor_spec_df     = spark.read.csv(\n",
    "    os.path.join(dim_path, \"Doctor_Speciality.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "operation_type_df  = spark.read.csv(\n",
    "    os.path.join(dim_path, \"OperationType.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "payer_types_df     = spark.read.csv(\n",
    "    os.path.join(dim_path, \"PayerTypes.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "service_catalog_df = spark.read.csv(\n",
    "    os.path.join(dim_path, \"ServiceCatalog.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "triage_levels_df   = spark.read.csv(\n",
    "    os.path.join(dim_path, \"TriageLevels.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff530c2-6985-4aad-ba9c-2f90e83e1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+------------+\n",
      "|SpecialityID|   SpecialityName|    Location|\n",
      "+------------+-----------------+------------+\n",
      "|           1|        Emergency|Ground Floor|\n",
      "|           2|Internal Medicine|   1st Floor|\n",
      "|           3|  General Surgery|   2nd Floor|\n",
      "|           4|      Orthopedics|   2nd Floor|\n",
      "|           5|       Pediatrics|   3rd Floor|\n",
      "+------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TypeID: integer (nullable = true)\n",
      " |-- TypeName: string (nullable = true)\n",
      " |-- ClassificationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "speciality_df.show(5)\n",
    "visit_type_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24858d55-2766-4e11-91cc-e007a0a6bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --------- Hospital 1 ----------\n",
    "patients_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"Patients.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "visits_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"Visits.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "admissions_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"Admissions.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "appointments_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"Appointments.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "bills_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"Bills.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "billitems_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"BillItems.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "emergency_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"EmergencyDetails.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "opsessions_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"OperationSessions.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "status_h1_df = spark.read.csv(\n",
    "    os.path.join(h1_path, \"VisitStatusHistory.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(1))\n",
    "\n",
    "# --------- Hospital 2 ----------\n",
    "patients_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"Patients.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "visits_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"Visits.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "admissions_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"Admissions.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "appointments_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"Appointments.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "bills_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"Bills.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "billitems_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"BillItems.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "emergency_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"EmergencyDetails.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "opsessions_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"OperationSessions.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))\n",
    "\n",
    "status_h2_df = spark.read.csv(\n",
    "    os.path.join(h2_path, \"VisitStatusHistory.csv\"),\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"HospitalID\", lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14075e91-6883-4cb8-9990-df1e69fd6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "visits_all_df = visits_h1_df.unionByName(visits_h2_df)\n",
    "patients_all_df = patients_h1_df.unionByName(patients_h2_df)\n",
    "admissions_all_df = admissions_h1_df.unionByName(admissions_h2_df)\n",
    "appointments_all_df = appointments_h1_df.unionByName(appointments_h2_df)\n",
    "bills_all_df = bills_h1_df.unionByName(bills_h2_df)\n",
    "billitems_all_df = billitems_h1_df.unionByName(billitems_h2_df)\n",
    "emergency_all_df = emergency_h1_df.unionByName(emergency_h2_df)\n",
    "opsessions_all_df = opsessions_h1_df.unionByName(opsessions_h2_df)\n",
    "status_all_df = status_h1_df.unionByName(status_h2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f8308d-68fd-4c26-bf74-3b350d1f07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def clean_str(col_name: str):\n",
    "    col = F.col(col_name)\n",
    "    return F.trim(F.regexp_replace(col, r\"\\s+\", \" \"))\n",
    "\n",
    "def normalize_gender(col_name: str):\n",
    "    c = F.lower(F.trim(F.col(col_name)))\n",
    "    return (\n",
    "        F.when(c.isNull(), \"O\")                                  ,
    "         .when(c.rlike(r\"^(m|male|man|boy)$\"), \"M\")\n",
    "         .when(c.rlike(r\"^(f|female|woman|girl)$\"), \"F\")\n",
    "         .otherwise(\"O\")                                          ,
    "    )\n",
    "\n",
    "def normalize_age(col_name: str):\n",
    "    c = F.col(col_name).cast(\"int\")\n",
    "    return (\n",
    "        F.when(c.isNull(), 0)                                     ,
    "         .when(c < 0, 0)                                        ,
    "         .when(c > 120, 120)                                    ,
    "         .otherwise(c)\n",
    "    )\n",
    "\n",
    "def normalize_double(col_name: str, default: float = 0.0):\n",
    "    c = F.col(col_name).cast(\"double\")\n",
    "    return F.when(c.isNull(), default).otherwise(c)\n",
    "\n",
    "def normalize_int(col_name: str, default=None):\n",
    "    c = F.col(col_name).cast(\"int\")\n",
    "    if default is None:\n",
    "        return c\n",
    "    else:\n",
    "        return F.when(c.isNull(), default).otherwise(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f53c17-d679-4057-8990-7aed2a7df95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+---+------+-----------+-----------------+----------+\n",
      "|PatientID|FirstName|LastName|Phone    |Age|Gender|Nationality|InsuranceProvider|HospitalID|\n",
      "+---------+---------+--------+---------+---+------+-----------+-----------------+----------+\n",
      "|1        |Patient1 |Example |101000001|21 |M     |Egyptian   |AXA Insurance    |1         |\n",
      "|2        |Patient2 |Example |101000002|22 |F     |Egyptian   |Allianz          |1         |\n",
      "|3        |Patient3 |Example |101000003|23 |M     |Egyptian   |Company Insurance|1         |\n",
      "|4        |Patient4 |Example |101000004|24 |F     |Egyptian   |NULL             |1         |\n",
      "|5        |Patient5 |Example |101000005|25 |M     |Egyptian   |AXA Insurance    |1         |\n",
      "+---------+---------+--------+---------+---+------+-----------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients_clean = (\n",
    "    patients_all_df\n",
    "    # IDs\n",
    "    .withColumn(\"PatientID\", F.col(\"PatientID\").cast(\"int\"))\n",
    "    .withColumn(\"HospitalID\", F.col(\"HospitalID\").cast(\"int\"))\n",
    "\n",
    "   ,
    "    .withColumn(\"FirstName\", clean_str(\"FirstName\"))\n",
    "    .withColumn(\"LastName\",  clean_str(\"LastName\"))\n",
    "    .withColumn(\"Phone\",     F.regexp_replace(\"Phone\", r\"[^\\d+]\", \"\")) ,
    "    .withColumn(\"Nationality\",      clean_str(\"Nationality\"))\n",
    "    .withColumn(\"InsuranceProvider\", clean_str(\"InsuranceProvider\"))\n",
    "\n",
    "    # Age / Gender normalization\n",
    "    .withColumn(\"Age\",    normalize_age(\"Age\"))\n",
    "    .withColumn(\"Gender\", normalize_gender(\"Gender\"))\n",
    ")\n",
    "\n",
    "patients_clean = patients_clean.where(F.col(\"PatientID\").isNotNull())\n",
    "\n",
    "patients_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94d19186-84ee-444a-8bde-557841435a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_clean = (\n",
    "    visits_all_df\n",
    "    .withColumn(\"HospitalID\",   F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"VisitID\",      F.col(\"VisitID\").cast(\"int\"))\n",
    "    .withColumn(\"PatientID\",    normalize_int(\"PatientID\"))\n",
    "    .withColumn(\"DoctorID\",     normalize_int(\"DoctorID\"))\n",
    "    .withColumn(\"ClassificationID\", normalize_int(\"ClassificationID\"))\n",
    "    .withColumn(\"TypeID\",       normalize_int(\"TypeID\"))\n",
    "    .withColumn(\"VisitDateTime\", safe_ts(\"VisitDate\"))\n",
    "    .withColumn(\"VisitDate\",\n",
    "                F.coalesce(F.to_date(\"VisitDateTime\"),\n",
    "                           F.to_date(\"VisitDate\")))  ,
    ")\n",
    "\n",
    "visits_clean = visits_clean.withColumn(\n",
    "    \"VisitDate\",\n",
    "    F.coalesce(\"VisitDate\", F.lit(\"2025-11-01\").cast(\"date\"))\n",
    ")\n",
    "\n",
    "visits_clean = (\n",
    "    visits_clean.alias(\"v\")\n",
    "    .join(\n",
    "        patients_clean.select(\"HospitalID\",\"PatientID\").alias(\"p\"),\n",
    "        on=[\"HospitalID\",\"PatientID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f07f88-2021-4fe7-8f61-63d9164006fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_clean = (\n",
    "    admissions_all_df\n",
    "    .withColumn(\"HospitalID\", F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"AdmissionID\", normalize_int(\"AdmissionID\"))\n",
    "    .withColumn(\"VisitID\",     normalize_int(\"VisitID\"))\n",
    "    .withColumn(\"BedID\",       normalize_int(\"BedID\", default=0))\n",
    "    .withColumn(\"AdmissionDateTime\", safe_ts(\"AdmissionDate\"))\n",
    "    .withColumn(\"DischargeDateTime\", safe_ts(\"DischargeDate\"))\n",
    ")\n",
    "\n",
    "admissions_clean = admissions_clean.withColumn(\n",
    "    \"DischargeDateTime\",\n",
    "    F.when(\n",
    "        (F.col(\"DischargeDateTime\").isNotNull()) &\n",
    "        (F.col(\"DischargeDateTime\") < F.col(\"AdmissionDateTime\")),\n",
    "        F.col(\"AdmissionDateTime\") ,
    "    ).otherwise(F.col(\"DischargeDateTime\"))\n",
    ")\n",
    "\n",
    "admissions_clean = admissions_clean.where(F.col(\"AdmissionID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32664703-5151-4451-b174-c4c124d74d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_clean = (\n",
    "    bills_all_df\n",
    "    .withColumn(\"HospitalID\", F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"BillID\",     normalize_int(\"BillID\"))\n",
    "    .withColumn(\"VisitID\",    normalize_int(\"VisitID\"))\n",
    "    .withColumn(\"AdmissionID\",normalize_int(\"AdmissionID\"))\n",
    "    .withColumn(\"PayerTypeID\",normalize_int(\"PayerTypeID\"))\n",
    "    .withColumn(\"BillDateTime\", safe_ts(\"BillDate\"))\n",
    "    .withColumn(\"BillDate\",\n",
    "                F.coalesce(F.to_date(\"BillDateTime\"),\n",
    "                           F.to_date(\"BillDate\")))\n",
    "    .withColumn(\"PaymentDateTime\", safe_ts(\"PaymentDate\"))\n",
    "    .withColumn(\"PaymentDate\",\n",
    "                F.coalesce(F.to_date(\"PaymentDateTime\"),\n",
    "                           F.to_date(\"PaymentDate\")))\n",
    "    .withColumn(\"TotalAmount\", normalize_double(\"TotalAmount\", default=0.0))\n",
    "    .withColumn(\"PaymentStatusRaw\", clean_str(\"PaymentStatus\"))\n",
    ")\n",
    "\n",
    "# Normalise PaymentStatus\n",
    "bills_clean = bills_clean.withColumn(\n",
    "    \"PaymentStatus\",\n",
    "    F.when(F.lower(\"PaymentStatusRaw\").isin(\"paid\", \"pay\", \"done\"), \"Paid\")\n",
    "     .when(F.lower(\"PaymentStatusRaw\").isin(\"pending\", \"awaiting\"), \"Pending\")\n",
    "     .when(F.lower(\"PaymentStatusRaw\").isin(\"cancel\", \"cancelled\", \"canceled\"), \"Cancelled\")\n",
    "     .otherwise(\"Pending\")  # default\n",
    ").drop(\"PaymentStatusRaw\")\n",
    "\n",
    "bills_clean = bills_clean.where(F.col(\"BillID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4035eee-2afd-4030-8d5b-69560fe7c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "billitems_clean = (\n",
    "    billitems_all_df\n",
    "    .withColumn(\"HospitalID\", F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"BillItemID\", normalize_int(\"BillItemID\"))\n",
    "    .withColumn(\"BillID\",     normalize_int(\"BillID\"))\n",
    "    .withColumn(\"ServiceID\",  normalize_int(\"ServiceID\"))\n",
    "    .withColumn(\"Quantity\",   normalize_int(\"Quantity\", default=1))\n",
    "    .withColumn(\"Price\",      normalize_double(\"Price\", default=0.0))\n",
    ")\n",
    "\n",
    "billitems_clean = billitems_clean.withColumn(\n",
    "    \"Quantity\",\n",
    "    F.when(F.col(\"Quantity\") <= 0, 1).otherwise(F.col(\"Quantity\"))\n",
    ").withColumn(\n",
    "    \"Price\",\n",
    "    F.when(F.col(\"Price\") < 0, 0.0).otherwise(F.col(\"Price\"))\n",
    ")\n",
    "\n",
    "billitems_clean = billitems_clean.where(F.col(\"BillItemID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc81220f-cab6-412b-8ac0-1a5a08f80d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_clean = (\n",
    "    status_all_df\n",
    "    .withColumn(\"HospitalID\", F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"HistoryID\",  normalize_int(\"HistoryID\"))\n",
    "    .withColumn(\"VisitID\",    normalize_int(\"VisitID\"))\n",
    "    .withColumn(\"StatusID\",   normalize_int(\"StatusID\"))\n",
    "    .withColumn(\"StatusTime\", safe_ts(\"StatusTime\"))\n",
    ")\n",
    "\n",
    "status_with_visit = (\n",
    "    status_clean.alias(\"h\")\n",
    "    .join(\n",
    "        visits_clean.select(\"HospitalID\",\"VisitID\",\"VisitDateTime\").alias(\"v\"),\n",
    "        on=[\"HospitalID\",\"VisitID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"StatusTime\",\n",
    "        F.coalesce(F.col(\"h.StatusTime\"), F.col(\"v.VisitDateTime\"))\n",
    "    )\n",
    "    .select(\"HospitalID\",\"HistoryID\",\"VisitID\",\"StatusID\",\"StatusTime\")\n",
    ")\n",
    "\n",
    "status_clean = status_with_visit.where(F.col(\"HistoryID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e997749c-0b81-4946-8190-a39a68f98cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_clean = (\n",
    "    appointments_all_df\n",
    "    .withColumn(\"HospitalID\",    F.col(\"HospitalID\").cast(\"int\"))\n",
    "    .withColumn(\"AppointmentID\", normalize_int(\"AppointmentID\"))\n",
    "    .withColumn(\"PatientID\",     normalize_int(\"PatientID\"))\n",
    "    .withColumn(\"DoctorID\",      normalize_int(\"DoctorID\"))\n",
    "    .withColumn(\"AppointmentDate\", F.to_date(\"AppointmentDate\"))\n",
    "    .withColumn(\"AppointmentTime\", clean_str(\"AppointmentTime\"))\n",
    "    .withColumn(\"Purpose\", clean_str(\"Purpose\"))\n",
    "    .withColumn(\"StatusRaw\", clean_str(\"Status\"))\n",
    ")\n",
    "\n",
    "appointments_clean = appointments_clean.withColumn(\n",
    "    \"Status\",\n",
    "    F.when(F.lower(\"StatusRaw\").isin(\"scheduled\", \"booked\"), \"Scheduled\")\n",
    "     .when(F.lower(\"StatusRaw\").isin(\"completed\", \"done\", \"finished\"), \"Completed\")\n",
    "     .when(F.lower(\"StatusRaw\").isin(\"cancelled\", \"canceled\", \"cancel\"), \"Cancelled\")\n",
    "     .when(F.lower(\"StatusRaw\").isin(\"no-show\", \"no show\", \"noshow\"), \"No-Show\")\n",
    "     .otherwise(\"Scheduled\")\n",
    ").drop(\"StatusRaw\")\n",
    "\n",
    "appointments_clean = appointments_clean.where(F.col(\"AppointmentID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10230a88-528d-423f-96d4-e58d597e063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------+---------+-------------------+-------------------+----------+\n",
      "|OperationSessionID|AdmissionID|OperationTypeID|SurgeonID|StartTime          |EndTime            |HospitalID|\n",
      "+------------------+-----------+---------------+---------+-------------------+-------------------+----------+\n",
      "|1                 |1          |1              |1        |2025-11-02 08:00:00|2025-11-02 09:15:00|1         |\n",
      "|2                 |2          |3              |1        |2025-11-05 07:30:00|2025-11-05 08:45:00|1         |\n",
      "|3                 |3          |5              |4        |2025-11-03 09:00:00|2025-11-03 11:00:00|1         |\n",
      "|4                 |4          |6              |6        |2025-11-06 10:00:00|2025-11-06 12:00:00|1         |\n",
      "|5                 |5          |2              |1        |2025-11-04 09:00:00|2025-11-04 11:00:00|1         |\n",
      "+------------------+-----------+---------------+---------+-------------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# base_path  = \"/home/jovyan/work/Hospital_Salam/Data\"\n",
    "# hosp1_path = f\"{base_path}/hospital1\"\n",
    "# hosp2_path = f\"{base_path}/hospital2\"\n",
    "\n",
    "ops_h1 = (\n",
    "    spark.read.csv(f\"{h1_path}/OperationSessions.csv\", header=True, inferSchema=True)\n",
    "    .withColumn(\"HospitalID\", F.lit(1))\n",
    ")\n",
    "\n",
    "ops_h2 = (\n",
    "    spark.read.csv(f\"{h2_path}/OperationSessions.csv\", header=True, inferSchema=True)\n",
    "    .withColumn(\"HospitalID\", F.lit(2))\n",
    ")\n",
    "\n",
    "operations_all_df = ops_h1.unionByName(ops_h2)\n",
    "\n",
    "operations_all_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92722bb4-9049-438b-9f58-44ecaa678e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------+---------+----------+-------------------+-------------------+\n",
      "|OperationSessionID|AdmissionID|OperationTypeID|SurgeonID|HospitalID|StartTime          |EndTime            |\n",
      "+------------------+-----------+---------------+---------+----------+-------------------+-------------------+\n",
      "|1                 |1          |1              |1        |1         |2025-11-02 08:00:00|2025-11-02 09:15:00|\n",
      "|2                 |2          |3              |1        |1         |2025-11-05 07:30:00|2025-11-05 08:45:00|\n",
      "|3                 |3          |5              |4        |1         |2025-11-03 09:00:00|2025-11-03 11:00:00|\n",
      "|4                 |4          |6              |6        |1         |2025-11-06 10:00:00|2025-11-06 12:00:00|\n",
      "|5                 |5          |2              |1        |1         |2025-11-04 09:00:00|2025-11-04 11:00:00|\n",
      "+------------------+-----------+---------------+---------+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ops_clean = (\n",
    "    operations_all_df\n",
    "    # Normalize IDs\n",
    "    .withColumn(\"HospitalID\",         normalize_int(\"HospitalID\"))\n",
    "    .withColumn(\"OperationSessionID\", normalize_int(\"OperationSessionID\"))\n",
    "    .withColumn(\"AdmissionID\",        normalize_int(\"AdmissionID\"))\n",
    "    .withColumn(\"OperationTypeID\",    normalize_int(\"OperationTypeID\"))\n",
    "    .withColumn(\"SurgeonID\",          normalize_int(\"SurgeonID\"))\n",
    ")\n",
    "\n",
    "ops_clean = (\n",
    "    ops_clean\n",
    "    .withColumn(\"StartTime_ts\", F.to_timestamp(\"StartTime\"))  ",
    "    .withColumn(\"EndTime_ts\",   F.to_timestamp(\"EndTime\"))\n",
    "    .withColumn(\n",
    "        \"EndTime_ts\",\n",
    "        F.when(\n",
    "            (F.col(\"EndTime_ts\").isNotNull()) & (F.col(\"EndTime_ts\") < F.col(\"StartTime_ts\")),\n",
    "            F.col(\"StartTime_ts\")\n",
    "        ).otherwise(F.col(\"EndTime_ts\"))\n",
    "    )\n",
    "    .drop(\"StartTime\", \"EndTime\")\n",
    "    .withColumnRenamed(\"StartTime_ts\", \"StartTime\")\n",
    "    .withColumnRenamed(\"EndTime_ts\",   \"EndTime\")\n",
    ")\n",
    "\n",
    "ops_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93bf7407-2c6a-4d0e-82c2-64781cbe6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-------------+------------------------------------------+----------+\n",
      "|EmergencyDetailID|VisitID|TriageLevelID|ChiefComplaint                            |HospitalID|\n",
      "+-----------------+-------+-------------+------------------------------------------+----------+\n",
      "|1                |6      |3            |High fever and confusion                  |1         |\n",
      "|2                |7      |2            |Road traffic accident with leg fracture   |1         |\n",
      "|3                |8      |1            |Severe chest pain with shortness of breath|1         |\n",
      "|4                |23     |2            |Severe abdominal pain and vomiting        |1         |\n",
      "|5                |26     |3            |Fall injury with wrist pain               |1         |\n",
      "+-----------------+-------+-------------+------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emerg_h1 = (\n",
    "    spark.read.csv(f\"{h1_path}/EmergencyDetails.csv\", header=True, inferSchema=True)\n",
    "    .withColumn(\"HospitalID\", F.lit(1))\n",
    ")\n",
    "\n",
    "emerg_h2 = (\n",
    "    spark.read.csv(f\"{h2_path}/EmergencyDetails.csv\", header=True, inferSchema=True)\n",
    "    .withColumn(\"HospitalID\", F.lit(2))\n",
    ")\n",
    "\n",
    "emergency_all_df = emerg_h1.unionByName(emerg_h2)\n",
    "\n",
    "emerg_clean = (\n",
    "    emergency_all_df\n",
    "    .withColumn(\"HospitalID\",        normalize_int(\"HospitalID\"))\n",
    "    .withColumn(\"EmergencyDetailID\", normalize_int(\"EmergencyDetailID\"))\n",
    "    .withColumn(\"VisitID\",           normalize_int(\"VisitID\"))\n",
    "    .withColumn(\"TriageLevelID\",     normalize_int(\"TriageLevelID\"))\n",
    "    .withColumn(\"ChiefComplaint\",    clean_str(\"ChiefComplaint\"))\n",
    "    .withColumn(\n",
    "        \"TriageLevelID\",\n",
    "        F.when(F.col(\"TriageLevelID\") < 1, 1)\n",
    "         .when(F.col(\"TriageLevelID\") > 5, 5)\n",
    "         .otherwise(F.col(\"TriageLevelID\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "emerg_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94fbbf35-1ca5-4b94-9ab9-a0d069a623b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_patients    = patients_clean\n",
    "stg_visits      = visits_clean\n",
    "stg_admissions  = admissions_clean\n",
    "stg_bills       = bills_clean\n",
    "stg_billitems   = billitems_clean\n",
    "stg_emergency   = emerg_clean\n",
    "stg_ops         = ops_clean\n",
    "stg_status      = status_clean\n",
    "stg_appointments= appointments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8460bc15-2c73-4248-a7c1-b0ee00a6c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "output_base = \"/home/jovyan/work/staging_csv\" ,
    "\n",
    "dfs = {\n",
    "    \"Patients\"      : stg_patients,\n",
    "    \"Visits\"        : stg_visits,\n",
    "    \"Admissions\"    : stg_admissions,\n",
    "    \"Bills\"         : stg_bills,\n",
    "    \"BillItems\"     : stg_billitems,\n",
    "    \"Emergency\"     : stg_emergency,\n",
    "    \"OperationSessions\": stg_ops,\n",
    "    \"VisitStatus\"   : stg_status,\n",
    "    \"Appointments\"  : stg_appointments,\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    (\n",
    "        df\n",
    "        .coalesce(1)                    ,
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"delimiter\", \",\")\n",
    "        .csv(f\"{output_base}/{name}\")   ,
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a6ccb5c-069c-4992-89ea-cc3f96af3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported /home/jovyan/work/staging_csv/Patients.csv\n",
      "Exported /home/jovyan/work/staging_csv/Visits.csv\n",
      "Exported /home/jovyan/work/staging_csv/Admissions.csv\n",
      "Exported /home/jovyan/work/staging_csv/Bills.csv\n",
      "Exported /home/jovyan/work/staging_csv/BillItems.csv\n",
      "Exported /home/jovyan/work/staging_csv/Emergency.csv\n",
      "Exported /home/jovyan/work/staging_csv/OperationSessions.csv\n",
      "Exported /home/jovyan/work/staging_csv/VisitStatus.csv\n",
      "Exported /home/jovyan/work/staging_csv/Appointments.csv\n"
     ]
    }
   ],
   "source": [
    "import os, glob, shutil\n",
    "\n",
    "for name in dfs.keys():\n",
    "    folder = f\"{output_base}/{name}\"\n",
    "    part_files = glob.glob(os.path.join(folder, \"part-*.csv\"))\n",
    "    if not part_files:\n",
    "        continue\n",
    "\n",
    "    part_file = part_files[0]\n",
    "    target_csv = os.path.join(output_base, f\"{name}.csv\")\n",
    "\n",
    "    shutil.move(part_file, target_csv)\n",
    "\n",
    "    shutil.rmtree(folder)\n",
    "\n",
    "    print(f\"Exported {target_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18587782-06fc-40aa-a32a-a8762c3133c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
hg